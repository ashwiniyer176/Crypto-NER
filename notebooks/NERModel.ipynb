{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel:\n",
    "    def __init__(self,vocabulary_file):\n",
    "        self.vocabulary=self.__load_vocabulary(vocabulary_file)\n",
    "        self.emoji_patterns = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                \"]+\", re.UNICODE)\n",
    "        self.url_pattern=re.compile(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\")\n",
    "        self.stops=set(stopwords.words(\"english\"))\n",
    "    def __load_vocabulary(self,vocabulary_file):\n",
    "        file=open(vocabulary_file)\n",
    "        return json.load(file)\n",
    "    def remove_regex_pattern(self,regex_pattern,text,replacement=\"\"):\n",
    "        text=re.sub(regex_pattern,replacement,text)\n",
    "        return text\n",
    "    def preprocess_text(self,text):\n",
    "        text=text.strip()\n",
    "        text=text.lower()\n",
    "        text=text.replace(\"\\n\",\" \")\n",
    "        text=self.remove_regex_pattern(self.emoji_patterns,text) # remove emojis\n",
    "        text=self.remove_regex_pattern(self.url_pattern,text) # remove urls\n",
    "        text=self.remove_regex_pattern(re.compile(\"[^a-z0-9%\\s']\"),text) # remove anything not an alphabet, a number, a space, a % or an apostrophe\n",
    "        text=self.remove_regex_pattern(re.compile(r\"\\s\\s+\"),text,replacement=\" \")\n",
    "        text=[word for word in text.split(\" \") if word not in self.stops] # stopword removal\n",
    "        text=\" \".join(text)\n",
    "        return text\n",
    "    \n",
    "    def predict(self,text):\n",
    "        text=self.preprocess_text(text)\n",
    "        doc=nlp(text)\n",
    "        answers=\"\"\n",
    "        for token in doc:\n",
    "            if token.pos_==\"NOUN\" or token.pos_==\"PROPN\":\n",
    "                if token.text in self.vocabulary:\n",
    "                    answers+=token.text+\" \"\n",
    "        if answers==\"\":\n",
    "            return np.nan\n",
    "        return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner=NERModel(\"../input/crypto_vocabulary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../input/cleaned_text.csv\")\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "cleantext    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"keywords\"]=df[\"cleantext\"].apply(lambda x: ner.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cleantext</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321712</td>\n",
       "      <td>hey using bot also filtering recommendations s...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321713</td>\n",
       "      <td>good stuff surprised took long find community lol</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321717</td>\n",
       "      <td>using non official one</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>321718</td>\n",
       "      <td>use one uniswap uses</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321719</td>\n",
       "      <td>keep mind hot subgraph change anytime without ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43481</th>\n",
       "      <td>374466</td>\n",
       "      <td>find many places also santiment historical bal...</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43482</th>\n",
       "      <td>374467</td>\n",
       "      <td>guys anyone know application tools able check ...</td>\n",
       "      <td>application transactions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43483</th>\n",
       "      <td>374468</td>\n",
       "      <td>lobsters going kyiv web3 hackathon september 6...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43484</th>\n",
       "      <td>374469</td>\n",
       "      <td>whats funny one complains txs rejected censore...</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43485</th>\n",
       "      <td>374470</td>\n",
       "      <td>way think chain analytics best user experience...</td>\n",
       "      <td>chain self</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43218 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                          cleantext  \\\n",
       "0      321712  hey using bot also filtering recommendations s...   \n",
       "1      321713  good stuff surprised took long find community lol   \n",
       "2      321717                             using non official one   \n",
       "3      321718                              use one uniswap uses    \n",
       "4      321719  keep mind hot subgraph change anytime without ...   \n",
       "...       ...                                                ...   \n",
       "43481  374466  find many places also santiment historical bal...   \n",
       "43482  374467  guys anyone know application tools able check ...   \n",
       "43483  374468  lobsters going kyiv web3 hackathon september 6...   \n",
       "43484  374469  whats funny one complains txs rejected censore...   \n",
       "43485  374470  way think chain analytics best user experience...   \n",
       "\n",
       "                        keywords  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  \n",
       "...                          ...  \n",
       "43481                      time   \n",
       "43482  application transactions   \n",
       "43483                        NaN  \n",
       "43484                       one   \n",
       "43485                chain self   \n",
       "\n",
       "[43218 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"keywords\"]!=None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['money money ', 'identity ', 'governance ', ...,\n",
       "       'year multi data ', 'application transactions ', 'chain self '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[pd.isna(df[\"keywords\"])==False][\"keywords\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../output/extracted_words.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "197295b20122369b6502e224a1410f8bc9b22cbbcad42d16b957c3154cf64235"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
